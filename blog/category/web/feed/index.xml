<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0"
     xmlns:content="http://purl.org/rss/1.0/modules/content/"
     xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
     xmlns:atom="http://www.w3.org/2005/Atom"
     xmlns:dc="http://purl.org/dc/elements/1.1/"
     xmlns:wfw="http://wellformedweb.org/CommentAPI/"
     >
  <channel>
    <title>mojavy.com</title>
    <link>http://mojavy.com/blog</link>
    <description></description>
    <pubDate>Tue, 23 Dec 2014 20:02:30 GMT</pubDate>
    <generator>Blogofile</generator>
    <sy:updatePeriod>hourly</sy:updatePeriod>
    <sy:updateFrequency>1</sy:updateFrequency>
    <item>
      <title>dnsimpleでダイナミックDNSをつかう</title>
      <link>http://mojavy.com/blog/2014/12/23/dnsimple-dynamic-dns/</link>
      <pubDate>Tue, 23 Dec 2014 20:02:30 JST</pubDate>
      <category><![CDATA[web]]></category>
      <category><![CDATA[dns]]></category>
      <guid isPermaLink="true">http://mojavy.com/blog/2014/12/23/dnsimple-dynamic-dns/</guid>
      <description>dnsimpleでダイナミックDNSをつかう</description>
      <content:encoded><![CDATA[<p><img alt="dnsimple" src="/images/dnsimple.png" /> </p>
<p><a href="https://dnsimple.com/r/4388f43fedebae">dnsimple</a> ではAPIからのDNSレコードアップデートができるので、簡単にダイナミックDNSがつかえる。 </p>
<p>手順は以下の通り。</p>
<ol>
<li>普通にdnsimpleでAレコードを登録する</li>
<li>登録したレコードのrecord idをしらべる。 record id は管理画面のURL <code>https://dnsimple.com/domains/example.com/records/&lt;record id&gt;/edit</code> をみればわかる。</li>
<li>レコードを更新するスクリプトをcronに登録する。 スクリプトは <a href="http://developer.dnsimple.com/ddns/">http://developer.dnsimple.com/ddns/</a> でダウンロードできる。 <code>RECORD_ID</code>には上記の値、<code>DOMAIN_ID</code>にはApexドメインを設定する。</li>
</ol>]]></content:encoded>
    </item>
    <item>
      <title>TCPの黒歴史：謎のオプションskeeterとbubbaについて</title>
      <link>http://mojavy.com/blog/2014/02/13/tcp-options-skeeter-bubba/</link>
      <pubDate>Thu, 13 Feb 2014 18:00:22 JST</pubDate>
      <category><![CDATA[web]]></category>
      <category><![CDATA[history]]></category>
      <guid isPermaLink="true">http://mojavy.com/blog/2014/02/13/tcp-options-skeeter-bubba/</guid>
      <description>TCPの黒歴史：謎のオプションskeeterとbubbaについて</description>
      <content:encoded><![CDATA[<p><img alt="ns" src="/images/netscape-150.png" /> </p>
<p>TCPについて調べてたら、<code>skeeter</code>と<code>bubba</code>という謎のオプションを見つけた。
調べてみたところ、TCPを策定した人達の黒歴史らしい。</p>
<ul>
<li><a href="http://mailman.postel.org/pipermail/internet-history/2001-November/000073.html">TCP options: Bubba and Skeeter</a> </li>
</ul>
<p>以下意訳です。</p>
<hr>

<p>あー、これは忘れられそうもないんだけど黒歴史だね。</p>
<p>ben levyとstevと私<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup> がFTPをやってたときにつくったものなんだ。
Bridghan<sup id="fnref:2"><a href="#fn:2" rel="footnote">2</a></sup>とstevがけしかけたんだけど、アイディアはシンプルで、鍵管理システムなしに暗号化できるように、デフィー・ヘルマン鍵共有をtcpで直接するためのものだった。
それには認証の仕組みは想定してなくて、パスワードみたいなものが共有されている前提だったんだ。
シンプルな実装で大体は動いてたんだけど、介入者攻撃には脆弱だったのが欠点だった。</p>
<p>なんで"skeeter"<sup id="fnref:3"><a href="#fn:3" rel="footnote">3</a></sup>  と"bubba"<sup id="fnref:4"><a href="#fn:4" rel="footnote">4</a></sup> かって？それはstevに聞いてくれ。</p>
<hr>

<p>なかなかセンスのあるネーミングですね。気になる人はstev氏に聞いてみてください。</p>
<p><strong>参考</strong></p>
<ul>
<li><a href="http://www.iana.org/assignments/tcp-parameters/tcp-parameters.xhtml">Transmission Control Protocol (TCP) Parameters</a> </li>
<li><a href="http://www.ietf.org/mail-archive/web/tcpm/current/msg05424.html">Re: [tcpm] IANA TCP options registry ...</a> </li>
</ul>
<div class="footnote">
<hr />
<ol>
<li id="fn:1">
<p>Kastenholz, Frank&#160;<a href="#fnref:1" rev="footnote" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:2">
<p>たぶんDave Bridgham のこと <a href="http://en.wikipedia.org/wiki/FTP_Software">http://en.wikipedia.org/wiki/FTP_Software</a>&#160;<a href="#fnref:2" rev="footnote" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
<li id="fn:3">
<p>トンボのことらしい&#160;<a href="#fnref:3" rev="footnote" title="Jump back to footnote 3 in the text">&#8617;</a></p>
</li>
<li id="fn:4">
<p>でかい人のことらしい&#160;<a href="#fnref:4" rev="footnote" title="Jump back to footnote 4 in the text">&#8617;</a></p>
</li>
</ol>
</div>]]></content:encoded>
    </item>
    <item>
      <title>あなたのgithub pagesを無料で高速化する方法</title>
      <link>http://mojavy.com/blog/2014/02/13/faster-github-pages/</link>
      <pubDate>Thu, 13 Feb 2014 09:10:00 JST</pubDate>
      <category><![CDATA[web]]></category>
      <category><![CDATA[github]]></category>
      <guid isPermaLink="true">http://mojavy.com/blog/2014/02/13/faster-github-pages/</guid>
      <description>あなたのgithub pagesを無料で高速化する方法</description>
      <content:encoded><![CDATA[<p><img alt="github" src="/images/github-logo-transparent-200.png" /> </p>
<p>このブログはgithub pages上に構築していますが、github pagesに引きずられて自分のブログも重くなるということが時々ありました。</p>
<p>がんばってブログを書いた次の日にアクセスできなくなってたりすると悲しいので何とか高速かつ安定した配信をする方法ないかなーと思って調べてみたところ、なんとgithub pagesに置いたコンテンツをCDNから配信させることができるようになったらしいです!</p>
<p><a href="https://github.com/blog/1715-faster-more-awesome-github-pages">Faster, More Awesome GitHub Pages</a> </p>
<p>どういうドメインでgithub pagesを配信しているかによって対応方法は違うので以下を読んで各自適切な設定をして下さい。</p>
<p><strong>目次</strong></p>
<div class="toc">
<ul>
<li><a href="#github-pages-usernamegithubio">デフォルトのgithub pagesのドメイン( username.github.io ) を使用している場合</a></li>
<li><a href="#wwwexamplecom">カスタムサブドメイン ( www.example.com ) を使用している場合</a></li>
<li><a href="#apex-examplecom">Apexドメイン ( example.com ) を使用している場合</a><ul>
<li><a href="#comdnsimple">お名前.comからDNSimpleに移行する場合</a><ul>
<li><a href="#1-dnsimple">1. DNSimpleに登録する</a></li>
<li><a href="#2-dnsimplegithub-pages">2. DNSimpleでgithub pagesとの連携を開始する</a></li>
<li><a href="#3">3. ネームサーバの設定をする</a></li>
<li><a href="#4-dns">4. dnsの更新を確認する</a></li>
<li><a href="#5-cdn">5. CDNの確認</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#_1">その他参考</a></li>
<li><a href="#_2">まとめ</a></li>
</ul>
</div>
<h2 id="github-pages-usernamegithubio">デフォルトのgithub pagesのドメイン( username.github.io ) を使用している場合</h2>
<p>既に対応しています。
なにもやる必要はありません。</p>
<h2 id="wwwexamplecom">カスタムサブドメイン ( www.example.com ) を使用している場合</h2>
<p>CNAMEの向き先をusername.github.io に向けるだけで対応できます。</p>
<h2 id="apex-examplecom">Apexドメイン ( example.com ) を使用している場合</h2>
<p>Apexドメインというのはサブドメインでない基本のドメイン部分のことで、そこを直接github pagesにIPに向くようにAレコードに設定している場合です。</p>
<p>この場合はDNSのプロバイダがALIASレコードをサポートしていれば、ALIASが <code>username.github.io</code> を向くようにすることで対応できます。</p>
<p>ALIASレコードに対応していない場合は残念ながらCDNを有効にできません。ちなみにお名前.comはALIASには対応していないので、その場合は他のDNSプロバイダに移行するしかありません。</p>
<h3 id="comdnsimple">お名前.comからDNSimpleに移行する場合</h3>
<p>このブログは残念ながらお名前.comでApexドメインをgithub pagesのIPに向けていたのでDNSimpleに移行することにしました。以下はお名前.comからDNSimpleに移行した際の手順です。</p>
<p>なお、この場合はタイトルに反して無料ではありません。</p>
<p>(ちなみにここではドメインの移管はせずに、DNSの設定だけをDNSimpleに移しました)</p>
<h4 id="1-dnsimple">1. DNSimpleに登録する</h4>
<p>DNSimpleはユーザビリティに主眼を置いたDNSサービスのようで<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup> 、APIでDNSレコードを更新したりAWSやHerokuやgithub pagesといったクラウドサービスとの連携が簡単にできます。</p>
<p><a href="https://dnsimple.com/r/4388f43fedebae">
<img alt="dnsimple" src="/images/dnsimple.png" /> </p>
<p><strong>DNSimple</strong>
</a></p>
<p>登録時に対象ドメインを聞かれるので、そのとき移行したいドメインを入力します。</p>
<p>ちなみに月額3ドルから<sup id="fnref:2"><a href="#fn:2" rel="footnote">2</a></sup> ですが30日は無料でつかえます。誰かを紹介すると、紹介した人とされた人の両方の無料期間が伸びます。上のリンクにはキャンペーンコードが埋め込まれているので、この記事を読んだ人はぜひ上のリンクから登録してください。</p>
<p>登録が完了するだけで一通りのUIが使えるようになりますが、DNS機能が有効になるのは決済情報を入力してからです。</p>
<h4 id="2-dnsimplegithub-pages">2. DNSimpleでgithub pagesとの連携を開始する</h4>
<p><a href="https://dnsimple.com/domains">dnsimpleの管理画面</a> に行くとServicesというボタンがあるのでそこからgithub pagesと連携を開始します。</p>
<h4 id="3">3. ネームサーバの設定をする</h4>
<p>お名前.com の以下のページの「他のネームサーバーを利用」からDNSimpleのネームサーバを指定します。</p>
<p><a href="https://www.onamae.com/domain/navi/ns_update/input?btn_id=navi_menu_domain_nsupdate_leftmenu_12">https://www.onamae.com/domain/navi/ns_update/input?btn_id=navi_menu_domain_nsupdate_leftmenu_12</a> </p>
<p>登録するサーバは以下です。</p>
<ul>
<li>ns1.dnsimple.com</li>
<li>ns2.dnsimple.com</li>
<li>ns3.dnsimple.com</li>
<li>ns4.dnsimple.com</li>
</ul>
<p><a href="http://support.dnsimple.com/articles/dnsimple-nameservers">参考 - DNSimple Name Servers</a> </p>
<p>この段階ではお名前.com側で設定しているAレコードはそのまま残しておきます。変更するのはネームサーバのみです。
そうしておかないと、ネームサーバの変更が伝搬するまでの間にお名前.comに問いあわせが来た場合NXDOMAIN扱いになってします。</p>
<h4 id="4-dns">4. dnsの更新を確認する</h4>
<p>更新前は以下</p>
<div class="pygments_borland"><pre>% dig mojavy.com
mojavy.com.             259     IN      A       207.97.227.245
</pre></div>

<p>もしくは以下</p>
<div class="pygments_borland"><pre>% dig mojavy.com
mojavy.com.             259     IN      A       204.232.175.78
</pre></div>

<p>のようになっていたはずですが、反映が完了すれば上記とは違うIPが返ってくるはずです。
反映までには時間がかかるので気長に待ちます。完全に反映されるには3日ほどかかるようです。</p>
<div class="pygments_borland"><pre>% dig mojavy.com
mojavy.com.             3600    IN      A       103.245.222.133

% dig taksatou.github.io
taksatou.github.io.     2973    IN      CNAME   github.map.fastly.net.
github.map.fastly.net.  15      IN      A       103.245.222.133
</pre></div>

<h4 id="5-cdn">5. CDNの確認</h4>
<p><a href="http://www.webpagetest.org/">webpagetest</a> のような計測ツールを使ってもいいですが、CDNからのレスポンスは<code>X-xxx</code>のようなヘッダがつくようなのでcurlでも確認できます。</p>
<div class="pygments_borland"><pre>% curl -v mojavy.com 2&gt;&amp;1  &gt; /dev/null  | grep &#39;X-&#39;
&lt; X-Served-By: cache-ty67-TYO
&lt; X-Cache: HIT
&lt; X-Cache-Hits: 5
&lt; X-Timer: S1392104522.668435574,VS0,VE0
</pre></div>

<p>もう完全に反映されたと思ったらお名前.com側で設定しているAレコードは無効にして大丈夫です。</p>
<h2 id="_1">その他参考</h2>
<ul>
<li><a href="https://help.github.com/articles/setting-up-a-custom-domain-with-pages">https://help.github.com/articles/setting-up-a-custom-domain-with-pages</a> </li>
</ul>
<h2 id="_2">まとめ</h2>
<ul>
<li>github pagesをつかえば無料でCDNから配信できる!</li>
<li>Apexドメインは慎重に使うべきだったという教訓を得た</li>
<li>DNSimple便利</li>
</ul>
<div class="footnote">
<hr />
<ol>
<li id="fn:1">
<p>The satisfaction of not having to use GoDaddy!  らしい&#160;<a href="#fnref:1" rev="footnote" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:2">
<p>登録ドメイン数に応じて自動的にプランが切りかわるようです。&#160;<a href="#fnref:2" rev="footnote" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>]]></content:encoded>
    </item>
    <item>
      <title>全文検索システムの比較 - Elasticsearch vs Solr vs Amazon CloudSearch</title>
      <link>http://mojavy.com/blog/2014/02/10/search-engine-comparison/</link>
      <pubDate>Mon, 10 Feb 2014 01:05:25 JST</pubDate>
      <category><![CDATA[solr]]></category>
      <category><![CDATA[aws]]></category>
      <category><![CDATA[elasticsearch]]></category>
      <category><![CDATA[web]]></category>
      <guid isPermaLink="true">http://mojavy.com/blog/2014/02/10/search-engine-comparison/</guid>
      <description>全文検索システムの比較 - Elasticsearch vs Solr vs Amazon CloudSearch</description>
      <content:encoded><![CDATA[<p>Elasticsearch、Solr、及び Amazon CloudSearchの比較検討を行った。</p>
<h2 id="_1">目次</h2>
<div class="toc">
<ul>
<li><a href="#_1">目次</a></li>
<li><a href="#_2">候補の選定方法</a></li>
<li><a href="#solr">Solr</a><ul>
<li><a href="#_3">長所</a></li>
<li><a href="#_4">短所</a></li>
</ul>
</li>
<li><a href="#elasticsearch">Elasticsearch</a><ul>
<li><a href="#_5">長所</a></li>
<li><a href="#_6">短所</a></li>
</ul>
</li>
<li><a href="#amazon-cloudsearch">Amazon CloudSearch</a><ul>
<li><a href="#_7">長所</a></li>
<li><a href="#_8">短所</a></li>
</ul>
</li>
<li><a href="#_9">比較項目別のまとめ</a><ul>
<li><a href="#_10">拡張性</a></li>
<li><a href="#_11">性能</a></li>
<li><a href="#_12">安定性</a></li>
<li><a href="#_13">リアルタイムデータ更新</a></li>
<li><a href="#_14">日本語対応</a></li>
<li><a href="#_15">スケーラビリティ</a></li>
</ul>
</li>
<li><a href="#_16">参考リンクまとめ</a></li>
<li><a href="#_17">所感</a></li>
</ul>
</div>
<h2 id="_2">候補の選定方法</h2>
<p>候補を選定するにあたって、以下の特徴をもっていることを前提とした。
LuceneやGroongaを使えば何でもできるが、ここでは対象としない。</p>
<ul>
<li>ウェブベースのインターフェースを持つ</li>
<li>インデックスの更新はほぼリアルタイムに反映される</li>
<li>スケールアウトが容易</li>
</ul>
<h2 id="solr">Solr</h2>
<p><img alt="solr" src="/images/solr-150.png" /> </p>
<p><a href="https://lucene.apache.org/solr/">https://lucene.apache.org/solr/</a> </p>
<p>Luceneをバックエンドにした全文検索システム。バージョン4になってから大幅に機能が増強された。</p>
<h3 id="_3">長所</h3>
<ul>
<li>実績が十分ある</li>
<li>機能豊富</li>
</ul>
<h3 id="_4">短所</h3>
<ul>
<li>クラスタを構築して運用するには手間がかかりそう</li>
<li>SolrCloudはzookeeperに依存するためサーバ台数もかさむ</li>
</ul>
<h2 id="elasticsearch">Elasticsearch</h2>
<p><img alt="Elasticsearch" src="/images/elasticsearch-logo.png" /> </p>
<p><a href="http://www.elasticsearch.org/">http://www.elasticsearch.org/</a> </p>
<p>Solrと同じくLuceneをバックエンドにした全文検索システム。開発者の言<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup>によると、Solrより洗練された分散モデルで<sup id="fnref:2"><a href="#fn:2" rel="footnote">2</a></sup> 、使いやすいAPIを備えている。 </p>
<h3 id="_5">長所</h3>
<ul>
<li>アーキテクチャやUIが今風</li>
<li>クラスタの構築が簡単</li>
<li>KibanaやLogstashと連携できる</li>
<li>Percolate APIというpush通知のような機能を簡単に実装するためのものがある</li>
</ul>
<h3 id="_6">短所</h3>
<ul>
<li>後発な分ノウハウの蓄積にやや不安が残る</li>
<li>未実装機能がいくらかある(あった。現時点(2014-02-09)では機能的にはほぼ追いついているように見える  <a href="http://solr-vs-elasticsearch.com/">http://solr-vs-elasticsearch.com/</a>  )</li>
</ul>
<h2 id="amazon-cloudsearch">Amazon CloudSearch</h2>
<p><img alt="amazon" src="/images/aws-logo-s.png" /> </p>
<p><a href="http://aws.amazon.com/jp/cloudsearch/">http://aws.amazon.com/jp/cloudsearch/</a> </p>
<p>AWS上で提供されている全文検索システム。EC2と同じく時間とトラフィックで課金される。現時点ではまだベータ。</p>
<h3 id="_7">長所</h3>
<ul>
<li>自動的にスケーリングしてくれる(エントリ数、リクエスト数に応じてインスタンスが自動的に増える)</li>
<li>pdfやdocをそのまま送るだけでも適当にうまくやってくれる</li>
<li>DynamoDBのデータをそのまま流してインデックスできる</li>
</ul>
<h3 id="_8">短所</h3>
<ul>
<li>現状では東京リージョンがない</li>
<li>テキスト解析のカスタマイズが限定的。現状、Stemming, Stopwords, Synonymsのみカスタム可能。</li>
<li>N-gramとか形態素解析は自前で処理してからアップロードする必要がある</li>
<li>ヒット位置を取る方法がない</li>
<li>テキスト本文をインデックスと一緒に格納することはできない</li>
</ul>
<h2 id="_9">比較項目別のまとめ</h2>
<h3 id="_10">拡張性</h3>
<p>SolrもElasticsearchもLuceneをバックエンドにしているので、Luceneでできることは基本的にはどちらでもできるはず。
Amazonは現状ではあまり拡張性はない。</p>
<h3 id="_11">性能</h3>
<p>基本性能はSolrもElasticsearchも大差はなさそう。
Amazonは自動的にノードが追加されるので性能の問題はなさそう。ただし、ノードが自動追加されるタイミングとその時の挙動は未確認。</p>
<h3 id="_12">安定性</h3>
<p>数年先行している分Solrがよいと思われるが、Elasticsearchも既に十分本番稼動実績はある。
Amazonはベータなので未知数。</p>
<h3 id="_13">リアルタイムデータ更新</h3>
<p>いずれもほぼリアルタイムに更新できる。</p>
<h3 id="_14">日本語対応</h3>
<p>SolrとElasticsearchはほぼ同等。kuromojiやmecabをつかえば形態素解析もできる。
Amazonはそれ自体では対応していないが、Luceneのtokenizer等を使って自前で前処理することで対応は可能。</p>
<h3 id="_15">スケーラビリティ</h3>
<p>Amazonは完全に自動的にスケールアウトしてくれる。
Elasticsearchはインデックスのシャード数を作成時に決めておく必要があるが、スケールアウトは容易だと思われる。
Solrはv4からはElasticsearchと大体同等のスケーラビリティを備えるようになった。</p>
<h2 id="_16">参考リンクまとめ</h2>
<ul>
<li><a href="http://stackoverflow.com/questions/10213009/solr-vs-elasticsearch">http://stackoverflow.com/questions/10213009/solr-vs-elasticsearch</a> </li>
<li><a href="http://stackoverflow.com/questions/2271600/elasticsearch-sphinx-lucene-solr-xapian-which-fits-for-which-usage">http://stackoverflow.com/questions/2271600/elasticsearch-sphinx-lucene-solr-xapian-which-fits-for-which-usage</a> </li>
<li><a href="http://blog.kakipo.com/trouble-with-fluentd-and-elasticsearch/">http://blog.kakipo.com/trouble-with-fluentd-and-elasticsearch/</a> </li>
<li><a href="https://github.com/atilika/kuromoji">https://github.com/atilika/kuromoji</a> </li>
<li><a href="http://www.elasticsearch.org/blog/percolator/">http://www.elasticsearch.org/blog/percolator/</a> </li>
<li><a href="http://blog.feedbin.me/2013/11/10/powering-actions-with-elasticsearch-percolate/">http://blog.feedbin.me/2013/11/10/powering-actions-with-elasticsearch-percolate/</a> </li>
<li><a href="http://docs.aws.amazon.com/cloudsearch/latest/developerguide/text-processing.html">http://docs.aws.amazon.com/cloudsearch/latest/developerguide/text-processing.html</a> </li>
<li><a href="http://blog.mikemccandless.com/2011/06/lucenes-near-real-time-search-is-fast.html">http://blog.mikemccandless.com/2011/06/lucenes-near-real-time-search-is-fast.html</a> </li>
<li><a href="https://wiki.apache.org/solr/Solr4.0">https://wiki.apache.org/solr/Solr4.0</a> </li>
<li><a href="http://www.slideshare.net/kucrafal/battle-of-the-giants-apache-solr-vs-elasticsearch">http://www.slideshare.net/kucrafal/battle-of-the-giants-apache-solr-vs-elasticsearch</a> </li>
</ul>
<h2 id="_17">所感</h2>
<p>後発な分Elasticsearchが一番洗練されているように思います。
Solrは無難に導入できそうですが、スケールアウトが必要になったとき手間がかかりそうです。
Amazonはメリットも多いですが、現状では制限が多いので使いづらいと思います。</p>
<p><strong>追記</strong></p>
<ul>
<li>2014/02/12 23:59:13： ElasticSearch →  Elasticsearchに直しました</li>
</ul>
<div class="footnote">
<hr />
<ol>
<li id="fn:1">
<p><a href="http://stackoverflow.com/questions/2271600/elasticsearch-sphinx-lucene-solr-xapian-which-fits-for-which-usage">http://stackoverflow.com/questions/2271600/elasticsearch-sphinx-lucene-solr-xapian-which-fits-for-which-usage</a> &#160;<a href="#fnref:1" rev="footnote" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:2">
<p>昔のSolrは単純なレプリケーションとシャーディングしかなかったので、クラスタを構築するのは大変だった&#160;<a href="#fnref:2" rev="footnote" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>]]></content:encoded>
    </item>
    <item>
      <title>ウェブエンジニアのためのオンラインツールまとめ</title>
      <link>http://mojavy.com/blog/2012/11/20/web-engineers-online-toolbox/</link>
      <pubDate>Tue, 20 Nov 2012 02:30:00 JST</pubDate>
      <category><![CDATA[web]]></category>
      <category><![CDATA[tips]]></category>
      <guid isPermaLink="true">http://mojavy.com/blog/2012/11/20/web-engineers-online-toolbox/</guid>
      <description>ウェブエンジニアのためのオンラインツールまとめ</description>
      <content:encoded><![CDATA[<p><img alt="tool" src="/images/tool-logo.png" /></p>
<p><a href="http://ivanzuzak.info/2012/11/18/the-web-engineers-online-toolbox.html">The Web engineer's online toolbox</a>というまとめ記事が便利そうだったので、実際に試しつつ抄訳してみました。(一部のコメントと体裁は変えています。)</p>
<h2 id="_1">目次</h2>
<div class="toc">
<ul>
<li><a href="#_1">目次</a></li>
<li><a href="#_2">一覧</a><ul>
<li><a href="#requestbin">RequestBin</a></li>
<li><a href="#hurl">Hurl</a></li>
<li><a href="#httpbin">httpbin</a></li>
<li><a href="#redbot">REDbot</a></li>
<li><a href="#webgun">WebGun</a></li>
<li><a href="#apify">Apify</a></li>
<li><a href="#unicorn">Unicorn</a></li>
<li><a href="#feed-validator">Feed validator</a></li>
<li><a href="#link-checker">Link checker</a></li>
<li><a href="#host-tracker">Host tracker</a></li>
<li><a href="#pingdom-full-page-test">Pingdom Full page test</a></li>
<li><a href="#har-viewer">HAR viewer</a></li>
<li><a href="#cors-proxy">CORS proxy</a></li>
<li><a href="#browserling">Browserling</a></li>
<li><a href="#websocket-echo-test">WebSocket Echo Test</a></li>
<li><a href="#yql">YQL</a></li>
<li><a href="#yahoo-pipes">Yahoo Pipes</a></li>
<li><a href="#apiary">Apiary</a></li>
</ul>
</li>
<li><a href="#_3">おわりに</a></li>
</ul>
</div>
<h2 id="_2">一覧</h2>
<h3 id="requestbin"><a href="http://requestb.in/">RequestBin</a></h3>
<p>httpリクエストを保存するエンドポイントを作ってくれる。</p>
<p>Create a RequestBin のボタンをクリックするとURLが表示されるので、そこをHTTPクライアントからたたくとRequestBin側にリクエスト内容が記録される。
ソースも公開されてるのでローカルで立ちあげることもできる。</p>
<p>githubの<a href="https://help.github.com/articles/testing-webhooks">webhookのhelp</a>も参考にどうぞ。</p>
<h3 id="hurl"><a href="http://hurl.it">Hurl</a></h3>
<p>httpリクエストを実行してくれる。パーマリンクも作ってくれるので、POSTリクエストもコピペで他の人と共有できる。</p>
<ul>
<li>類似サービス: <a href="http://resttesttest.com/">REST test test</a> ,  <a href="https://apigee.com/console/others">Apigee console</a> </li>
</ul>
<h3 id="httpbin"><a href="http://httpbin.org/">httpbin</a></h3>
<p>HTTPリクエスト側でレスポンスのHTTP status codeやレスポンスやリダイレクト、cookieなどを制御できる。HTTPクライアントのテストに便利。</p>
<ul>
<li>類似サービス: <a href="http://ivanzuzak.info/urlecho/">UrlEcho</a></li>
</ul>
<h3 id="redbot"><a href="http://redbot.org/">REDbot</a></h3>
<p>HTTPのリソースをチェックしてくれる。問題を検出したら改善案のサジェストもしてくれる。</p>
<ul>
<li>類似サービス: <a href="http://zamez.org/httplint">HTTP lint</a></li>
</ul>
<h3 id="webgun"><a href="http://webgun.io/">WebGun</a></h3>
<p>webhookを簡単に作るためのAPIを提供している。現在はまだベータっぽい。</p>
<h3 id="apify"><a href="http://apify.heroku.com">Apify</a></h3>
<p>ウェブページをスクレイピングしてJSON API化してくれる。
CSSセレクタかxpathに名前をつけて設定するだけでよしなにやってくれる。</p>
<p><a href="http://apify.heroku.com/resources/50a99278a7301c000200005a">試しに元記事をAPI化してみた</a></p>
<h3 id="unicorn"><a href="http://validator.w3.org/unicorn/">Unicorn</a></h3>
<p>前述の<a href="http://redbot.org/">REDbot</a>はHTTPをチェックしてくれるけど、こちらはHTMLドキュメントがW3Cに準拠してるかをバリデートしてくれる。</p>
<ul>
<li>類似サービス: <a href="http://lint.brihten.com/html/">HTML lint</a></li>
</ul>
<h3 id="feed-validator"><a href="http://validator.w3.org/feed/">Feed validator</a></h3>
<p>こちらはRSSとATOMフィードをバリデートしてくれる。</p>
<h3 id="link-checker"><a href="http://validator.w3.org/checklink">Link checker</a></h3>
<p>リンクを再帰的にたどって、重複やリンク切れをチェックしてくれる。</p>
<h3 id="host-tracker"><a href="http://www.host-tracker.com/">Host tracker</a></h3>
<p>ウェブサイトのモニタリングサービス。定期的にpingして、問題があった場合はメール通知してくれる。</p>
<ul>
<li>類似サービス: <a href="http://www.downforeveryoneorjustme.com/">Down for everyone or just me</a>, <a href="http://tools.pingdom.com/ping/">Pimgdom ping service</a></li>
</ul>
<h3 id="pingdom-full-page-test"><a href="http://tools.pingdom.com/fpt/">Pingdom Full page test</a></h3>
<p>ウェブページのロード時間を計測、解析してくれる。リクエストを実行するサーバは米国しかないので国内だとちょっと使いづらい。
類似サービスの<a href="http://www.webpagetest.org/">Web page test</a>ならTokyoが選べる。</p>
<h3 id="har-viewer"><a href="http://www.softwareishard.com/har/viewer/">HAR viewer</a></h3>
<p>HTTP tracking toolsで生成されたHTTP Archive (HAR)形式のログをビジュアライズしてくれる。</p>
<h3 id="cors-proxy"><a href="http://www.corsproxy.com/">CORS proxy</a></h3>
<p>クロスドメインでjsを実行できるようにヘッダを追加してレスポンスしてくれるプロキシ。</p>
<h3 id="browserling"><a href="https://browserling.com/">Browserling</a></h3>
<p>ブラウザ上で動くインタラクティブなクロスブラウザ。メジャーなブラウザは大体サポートされている。
動作は重いけど表示確認くらいになら無料のままでも使えそう。</p>
<h3 id="websocket-echo-test"><a href="http://www.websocket.org/echo.html">WebSocket Echo Test</a></h3>
<p>WebSocketをテストできるエコーサーバ。</p>
<h3 id="yql"><a href="http://developer.yahoo.com/yql/">YQL</a></h3>
<p>ウェブサービスからとってきたデータをSQLっぽい言語で整形できる。</p>
<h3 id="yahoo-pipes"><a href="http://pipes.yahoo.com/pipes/">Yahoo Pipes</a></h3>
<p>ウェブサービスをGUIでマッシュアップできる。</p>
<h3 id="apiary"><a href="http://apiary.io/">Apiary</a></h3>
<p>良い感じのREST APIドキュメントをインタラクティブなインスペクタをつかってジェネレートできる。まだベータ。</p>
<ul>
<li>類似サービス: <a href="http://swagger.wordnik.com/">Swagger</a></li>
</ul>
<h2 id="_3">おわりに</h2>
<ul>
<li>いくつかのツールはサーバに負荷をかけてしまう可能性があるので注意してつかってください。</li>
<li>他にもなにかあれば教えてもらえるとうれしいです。</li>
</ul>]]></content:encoded>
    </item>
    <item>
      <title>twitter apiで404: {"errors":[{"message":"Sorry, that page does not exist","code":34}]}</title>
      <link>http://mojavy.com/blog/2012/10/18/twitter-api-error/</link>
      <pubDate>Thu, 18 Oct 2012 22:00:00 JST</pubDate>
      <category><![CDATA[web]]></category>
      <category><![CDATA[twitter]]></category>
      <guid isPermaLink="true">http://mojavy.com/blog/2012/10/18/twitter-api-error/</guid>
      <description>twitter apiで404: {"errors":[{"message":"Sorry, that page does not exist","code":34}]}</description>
      <content:encoded><![CDATA[<p><img alt="twitter" src="/images/twitter-logo.png" /></p>
<p>最近twitter apiをつかっているサービスで以下のようなエラーがでた。</p>
<div class="pygments_borland"><pre><span class="p">{</span><span class="s2">&quot;errors&quot;</span><span class="o">:</span><span class="p">[{</span><span class="s2">&quot;message&quot;</span><span class="o">:</span><span class="s2">&quot;Sorry, that page does not exist&quot;</span><span class="p">,</span><span class="s2">&quot;code&quot;</span><span class="o">:</span><span class="mi">34</span><span class="p">}]}</span>
</pre></div>

<p><a href="https://dev.twitter.com/discussions/11595">twitterのデベロッパフォーラム</a>によると、先週くらいにバージョンがついてないエンドポイントは /oauth を除いて廃止されたらしい。また、ドメインもapi.twitter.comに統一されたらしい。</p>
<p>つまり、api.twitter.com/account/verify_credentials.json とか twitter.com/1/account/verify_credentials.json とか twitter.com/account/verify_credentials.json ではだめで、api.twitter.com/1/account/verify_credentials.json を使えということ。</p>
<p>ログイン直後にverify_credentialsをつかってるアプリは多いと思われるが、その場合ログインに失敗したように見えるので認証まわりをライブラリ任せにしてると原因がわかりにくいかも。</p>
<p>とりあえず<a href="https://dev.twitter.com/blog">twitterの開発ブログ</a>まめにチェックしたほうがよさそう。</p>
<h1 id="_1">参考</h1>
<ul>
<li><a href="https://dev.twitter.com/blog">https://dev.twitter.com/blog</a></li>
<li><a href="https://dev.twitter.com/discussions/11595">https://dev.twitter.com/discussions/11595</a></li>
</ul>]]></content:encoded>
    </item>
    <item>
      <title>ワンライナーでウェブサーバを起動する方法</title>
      <link>http://mojavy.com/blog/2012/07/18/one-line-webserver/</link>
      <pubDate>Wed, 18 Jul 2012 12:30:00 JST</pubDate>
      <category><![CDATA[python]]></category>
      <category><![CDATA[web]]></category>
      <category><![CDATA[ruby]]></category>
      <guid isPermaLink="true">http://mojavy.com/blog/2012/07/18/one-line-webserver/</guid>
      <description>ワンライナーでウェブサーバを起動する方法</description>
      <content:encoded><![CDATA[<p><img alt="ruby" src="/images/ruby-logo.png" /></p>
<p>とりあえずウェブサーバがたちあがりさえすればいいときは、pythonのSimpleHTTPServerを使うのが便利です。
起動したカレントディレクトリ以下のファイルをブラウズできるので、テスト用のスタティックなスタブデータを一時的に配置したいときとかにも使えます。最近の一般的なlinuxディストリビューションであればデフォルトではいってるpythonで使えると思います。</p>
<div class="pygments_borland"><pre><span class="nv">$ </span>python -mSimpleHTTPServer 3333
</pre></div>

<p>デフォルトポートは8000ですが、引数で指定することもできます。
<br>
ちなみにrubyでもwebrickを使って同様のことができますが、<a href="http://d.hatena.ne.jp/rx7/20090812/p1">こちら</a> で紹介されているwebrickのワンライナーは長すぎて覚えられないのでいつもpythonを使ってます。
<br>
<br>
でもリクエストに応じたロジックを入れたい場合はrubyのsinatraの方が便利です。</p>
<div class="pygments_borland"><pre><span class="nv">$ </span>ruby -rsinatra -e <span class="s1">&#39;get(&quot;/&quot;){sleep 3}&#39;</span>
</pre></div>

<p>ポートを変更する場合は以下のようにします</p>
<div class="pygments_borland"><pre><span class="nv">$ </span>ruby -rsinatra -e <span class="s1">&#39;set :port,3333; get(&quot;/&quot;){sleep 3}&#39;</span>
</pre></div>

<h2 id="_1">参考</h2>
<ul>
<li><a href="http://d.hatena.ne.jp/rx7/20090812/p1">コマンド1つで今すぐWebサーバを起動させるためのワンライナー(Ruby or Python)</a></li>
</ul>]]></content:encoded>
    </item>
  </channel>
</rss>
